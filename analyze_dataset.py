"""
Analyze the synthetic dataset to assess if it's sufficient for ML training
"""

import pandas as pd
import numpy as np
from collections import Counter

# Load the dataset
df = pd.read_csv('ml/data/fitness_dataset.csv')

print("=" * 70)
print("DATASET SUFFICIENCY ANALYSIS")
print("=" * 70)

# 1. BASIC STATISTICS
print("\nüìä 1. DATASET SIZE")
print(f"   Total samples: {len(df):,}")
print(f"   Features: {len(df.columns) - 1} (excluding target)")
print(f"   Target variable: 'next_workout'")

# Rule of thumb: 
# - Minimum: 100 samples per class
# - Good: 1000+ samples per class
# - Excellent: 5000+ samples per class

target_counts = df['next_workout'].value_counts()
print(f"\n   Target classes: {len(target_counts)}")
print(f"   Min samples per class: {target_counts.min():,}")
print(f"   Max samples per class: {target_counts.max():,}")
print(f"   Avg samples per class: {target_counts.mean():.0f}")

# 2. CLASS BALANCE
print("\n‚öñÔ∏è  2. CLASS BALANCE")
print("\n   Class distribution:")
for workout, count in target_counts.items():
    percentage = (count / len(df)) * 100
    bar = "‚ñà" * int(percentage / 2)
    status = "‚úì" if count >= 100 else "‚ö†Ô∏è"
    print(f"   {status} {workout:12s}: {count:4,} ({percentage:5.1f}%) {bar}")

# Check imbalance ratio
max_class = target_counts.max()
min_class = target_counts.min()
imbalance_ratio = max_class / min_class
print(f"\n   Imbalance ratio: {imbalance_ratio:.1f}:1")
if imbalance_ratio < 3:
    print("   ‚úÖ Well balanced (ratio < 3:1)")
elif imbalance_ratio < 10:
    print("   ‚ö†Ô∏è  Moderate imbalance (ratio 3:1 - 10:1)")
else:
    print("   ‚ùå High imbalance (ratio > 10:1) - may need resampling")

# 3. FEATURE COVERAGE
print("\nüîç 3. FEATURE COVERAGE")
print("\n   Missing values:")
missing = df.isnull().sum()
if missing.sum() == 0:
    print("   ‚úÖ No missing values!")
else:
    for col, count in missing[missing > 0].items():
        print(f"   ‚ö†Ô∏è  {col}: {count} missing ({count/len(df)*100:.1f}%)")

print("\n   Feature types:")
for col in df.columns:
    if col == 'next_workout':
        continue
    dtype = df[col].dtype
    unique = df[col].nunique()
    print(f"   ‚Ä¢ {col:25s}: {str(dtype):10s} ({unique:4d} unique values)")

# 4. DATA QUALITY
print("\nüéØ 4. DATA QUALITY CHECKS")

# Check for duplicate rows
duplicates = df.duplicated().sum()
print(f"   Duplicate rows: {duplicates} ({duplicates/len(df)*100:.1f}%)")
if duplicates == 0:
    print("   ‚úÖ No duplicates")
else:
    print("   ‚ö†Ô∏è  Consider removing duplicates")

# Check for constant features
print("\n   Feature variance:")
numeric_cols = df.select_dtypes(include=[np.number]).columns
for col in numeric_cols:
    if col == 'days_since_last_workout':
        continue
    variance = df[col].var()
    if variance == 0:
        print(f"   ‚ùå {col}: No variance (constant feature)")
    else:
        print(f"   ‚úÖ {col}: Has variance")

# 5. SAMPLE SIZE ASSESSMENT
print("\nüìè 5. SAMPLE SIZE ASSESSMENT")
print("\n   ML Model Requirements:")
print(f"   ‚Ä¢ Current dataset: {len(df):,} samples")
print(f"   ‚Ä¢ Train/test split (80/20): {int(len(df)*0.8):,} / {int(len(df)*0.2):,}")
print(f"   ‚Ä¢ Samples per class (train): ~{int(target_counts.mean()*0.8)}")

# Rule of thumb for classification:
# - Minimum viable: 10 * features * classes
# - Good: 50 * features * classes
# - Excellent: 100+ * features * classes

n_features = len(df.columns) - 1
n_classes = len(target_counts)
min_samples = 10 * n_features * n_classes
good_samples = 50 * n_features * n_classes
excellent_samples = 100 * n_features * n_classes

print(f"\n   Recommended sample sizes (for {n_features} features, {n_classes} classes):")
print(f"   ‚Ä¢ Minimum viable:  {min_samples:,} samples")
print(f"   ‚Ä¢ Good:            {good_samples:,} samples")
print(f"   ‚Ä¢ Excellent:       {excellent_samples:,} samples")
print(f"   ‚Ä¢ Your dataset:    {len(df):,} samples")

if len(df) >= excellent_samples:
    status = "‚úÖ EXCELLENT"
elif len(df) >= good_samples:
    status = "‚úÖ GOOD"
elif len(df) >= min_samples:
    status = "‚ö†Ô∏è  MINIMUM VIABLE"
else:
    status = "‚ùå INSUFFICIENT"

print(f"\n   Status: {status}")

# 6. FINAL VERDICT
print("\n" + "=" * 70)
print("FINAL VERDICT")
print("=" * 70)

issues = []
recommendations = []

# Check critical issues
if len(df) < min_samples:
    issues.append("‚ùå Dataset too small")
    recommendations.append(f"Generate at least {min_samples:,} samples")
elif len(df) < good_samples:
    issues.append("‚ö†Ô∏è  Dataset size is minimum viable but not ideal")
    recommendations.append(f"Consider increasing to {good_samples:,}+ samples for better accuracy")

if imbalance_ratio > 10:
    issues.append("‚ùå High class imbalance")
    recommendations.append("Apply SMOTE or class weighting to balance classes")
elif imbalance_ratio > 3:
    issues.append("‚ö†Ô∏è  Moderate class imbalance")
    recommendations.append("Consider using class_weight='balanced' in RandomForest")

if min_class < 100:
    issues.append(f"‚ö†Ô∏è  Some classes have < 100 samples")
    recommendations.append("Generate more samples for underrepresented classes")

# Print results
if not issues:
    print("\n‚úÖ DATASET IS SUFFICIENT FOR TRAINING!")
    print("\n   Your dataset meets all requirements for training a RandomForest model.")
    print(f"   Expected baseline accuracy: 65-75%")
    print(f"   With tuning: 75-85%")
else:
    print("\n‚ö†Ô∏è  DATASET HAS SOME ISSUES:")
    for issue in issues:
        print(f"   {issue}")
    
    print("\nüí° RECOMMENDATIONS:")
    for i, rec in enumerate(recommendations, 1):
        print(f"   {i}. {rec}")

# 7. ACTIONABLE SUGGESTIONS
print("\n" + "=" * 70)
print("ACTIONABLE NEXT STEPS")
print("=" * 70)

print("\n1Ô∏è‚É£  CURRENT STATE:")
if len(df) >= good_samples and imbalance_ratio < 3 and min_class >= 100:
    print("   ‚úÖ You can proceed with model training!")
    print("   ‚úÖ Dataset quality is good for baseline model")
elif len(df) >= min_samples:
    print("   ‚ö†Ô∏è  Dataset is usable but has room for improvement")
    print("   ‚úÖ You can start training and iterate")
else:
    print("   ‚ùå Dataset needs improvement before training")

print("\n2Ô∏è‚É£  QUICK IMPROVEMENTS (if needed):")
if len(df) < good_samples:
    print(f"   ‚Ä¢ Increase n_users to 150 or days_per_user to 120")
    print(f"     Target: {good_samples:,}+ samples")

if imbalance_ratio > 3:
    print("   ‚Ä¢ Use class_weight='balanced' in RandomForest")
    print("   ‚Ä¢ Or apply SMOTE oversampling to minority classes")

if min_class < 100:
    print("   ‚Ä¢ Generate more data for underrepresented classes")
    print(f"   ‚Ä¢ Classes with < 100 samples: {list(target_counts[target_counts < 100].index)}")

print("\n3Ô∏è‚É£  TRAINING STRATEGY:")
print("   ‚úÖ Use train_test_split with stratify=y")
print("   ‚úÖ Start with RandomForestClassifier (robust to small data)")
print("   ‚úÖ Use cross-validation (5-fold) to validate performance")
print("   ‚úÖ Monitor both training and test accuracy to detect overfitting")

print("\n" + "=" * 70)
